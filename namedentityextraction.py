# -*- coding: utf-8 -*-
"""NamedEntityExtraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AfGOA7nzxdlEWPP0ZvyJjMFhhMB7xlrF
"""

# Commented out IPython magic to ensure Python compatibility.
# %reset -f

!pip install pyldavis
!pip install guidedlda

import pandas as pd
import numpy as np
import spacy
import gensim
import nltk 

from nltk.corpus import stopwords
nltk.download('stopwords')
from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel
from gensim.models.wrappers import LdaMallet
from gensim.corpora import Dictionary
from gensim.utils import simple_preprocess
import pyLDAvis.gensim
#import seaborn as sns
#sns.set()
import os, re, operator, warnings
warnings.filterwarnings('ignore')  
#from google.colab import drive 
#drive.mount('/content/gdrive')

# Read data 
#df= pd.read_csv("/content/gdrive/My Drive/Articles.csv",encoding= 'unicode_escape')
df= pd.read_csv("/content/Articles.csv",encoding= 'unicode_escape')

print(set(df['NewsType']))
df = df.drop(columns=['Date', 'Heading', 'NewsType'], axis=1)
#df = df.sample(100)
df.head(100)

#Pre-Processing 

# Remove punctuations
df['Article_Pre_Processed'] = df['Article'].map(lambda x: re.sub('[,\.!?]', '', x))

# Convert to lowercase
df['Article_Pre_Processed'] = df['Article_Pre_Processed'].map(lambda x: x.lower())

# Print 
df['Article_Pre_Processed'].head()

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

data = df.Article_Pre_Processed.values.tolist()
data
#List of words
data_words = list(sent_to_words(data))

print(data_words[:1][0][:30])

# Build the bigram and trigram models
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  

#Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

#Saving stop-words from NLTK library
stop_words = stopwords.words('english')

# Functions for removing stopwords, making bigrams, trigrams and doing lemmatization
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

# Using the created functions

#Remove Stopwords
data_words_nostops = remove_stopwords(data_words)

# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
nlp = spacy.load("en_core_web_sm", disable=['parser', 'ner'])

# Do lemmatization keeping only noun, adj, vb, adv
data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

print(data_lemmatized[:1][0][:30])

import gensim.corpora as corpora
# Create Dictionary
id2word = corpora.Dictionary(data_lemmatized)

# Create Corpus
texts = data_lemmatized

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
# View
print(corpus[:1][0][:30])
#corpus.shape

#Making the LSI Model
lsimodel = LsiModel(corpus=corpus, num_topics=10, id2word=id2word)
lsimodel.show_topics()

#Making the HDP Model
hdpmodel = HdpModel(corpus=corpus, id2word=id2word)
hdpmodel.show_topics()

#Making the LDA Model
# corpus.shape
ldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=id2word)
ldamodel.show_topics()

#Making the LDA Model with 3 topics
ldamodel_3_topics = LdaModel(corpus=corpus, num_topics=3, id2word=id2word)
ldamodel_3_topics.show_topics()

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(ldamodel, corpus, id2word)

lsitopics = [[word for word, prob in topic] for topicid, topic in lsimodel.show_topics(formatted=False)]

hdptopics = [[word for word, prob in topic] for topicid, topic in hdpmodel.show_topics(formatted=False)]

ldatopics = [[word for word, prob in topic] for topicid, topic in ldamodel.show_topics(formatted=False)]

lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=texts, dictionary=id2word, window_size=10).get_coherence()
print("The value of LSI coherence Score:", lsi_coherence)

hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=texts, dictionary=id2word, window_size=10).get_coherence()
print("The value of HDP coherence Score:",hdp_coherence)

lda_coherence = CoherenceModel(topics=ldatopics, texts=texts, dictionary=id2word, window_size=10).get_coherence()
print("The value of LDA coherence Score :", lda_coherence)

import matplotlib.pyplot as plt
def evaluate_bar_graph(coherences, indices):
    assert len(coherences) == len(indices)
    n = len(coherences)
    x = np.arange(n)
    plt.bar(x, coherences,  tick_label=indices, align='center',color=['grey', 'green', 'blue'])
    csfont = {'fontname':'Comic Sans MS'}
    #hfont = {'fontname':'Helvetica'}
    plt.xlabel('TOPIC MODELS',color="RED",**csfont)
    plt.ylabel('COHERENCE VALUES',color="GREEN",**csfont)

evaluate_bar_graph([lsi_coherence, hdp_coherence, lda_coherence],
['LSI-SCORE', 'HDP-SCORE', 'LDA-SCORE'])

##### GUIDED LDA #######
#### Semi Supervised Approach to Topic Modeling using Guided LDA ####

import guidedlda

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['Article_Pre_Processed'])
#word2id = dict((v, idx) for idx, v in enumerate(df['Article_Pre_Processed']))
X = np.transpose(X)
print(type(X))
X=(X.toarray())
X.shape
print(type(X))
final_list = []
for list in texts:
  final_list = final_list+list

fl = set(final_list)
print("asdfasdfa",len(fl))
word2id = dict((v, idx) for idx, v in enumerate(final_list))

#Regular LDA Model to see the topics
model_unguided = guidedlda.GuidedLDA(n_topics=10, n_iter=100, random_state=7, refresh=20)
model_unguided.fit(X)
model_unguided.word_topic_

topic_word = model_unguided.topic_word_
n_top_words = 10
for i, topic_dist in enumerate(topic_word):
     topic_words = np.array(final_list)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
     print('Topic {}: {}'.format(i, ' '.join(topic_words)))

#Guided LDA Model
#Seed topic list to guide the model
seed_topic_list = [['datum','lose','corporate', 'fuel', 'rent', 'company', 'affect', 'risk'],
                   ['crude', 'share','market','oil', 'cut', 'index', 'loss', 'sink', 'slip'],
                   ['note', 'accord','committee','displace','crisis','government','management']]
model_guided = guidedlda.GuidedLDA(n_topics=10, n_iter=100, random_state=7, refresh=20)

seed_topics = {}
for t_id, st in enumerate(seed_topic_list):
    for word in st:
        seed_topics[word2id[word]] = t_id

model_guided.fit(X, seed_topics=seed_topic_list, seed_confidence=0.50)

n_top_words = 10
topic_word = model_guided.topic_word_
for i, topic_dist in enumerate(topic_word):
    topic_words = np.array(final_list)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
    print('Topic {}: {}'.format(i, ' '.join(topic_words)))